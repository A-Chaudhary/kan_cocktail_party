{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "import torchaudio\n",
    "from kan import KAN\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torchaudio.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.path.isfile('./data/Dipco/audio/dev/S10_P29.wav'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of waveform: torch.Size([1, 19349333])\n",
      "Sample rate: 16000\n"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "\n",
    "# Load the audio file\n",
    "waveform, sample_rate = torchaudio.load('./data/Dipco/audio/dev/S10_P29.wav')\n",
    "\n",
    "# Print the shape of the waveform tensor\n",
    "print(\"Shape of waveform:\", waveform.shape)\n",
    "\n",
    "# Print the sample rate\n",
    "print(\"Sample rate:\", sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiPCoSeparationDataset(Dataset):\n",
    "    def __init__(self, root_dir, session_ids, version='dev', \n",
    "                 segment_length=3, sr=16000, max_pairs=1000, \n",
    "                 n_mels=128, mel_reduction='mean'):\n",
    "        self.sr = sr\n",
    "        self.segment_samples = int(sr * segment_length)\n",
    "        self.n_mels = n_mels\n",
    "\n",
    "        self.mel_reduction = mel_reduction\n",
    "        self.session_data = {}\n",
    "\n",
    "        self.mel_transform = T.MelSpectrogram(\n",
    "            sample_rate=sr,\n",
    "            n_fft=1024,\n",
    "            hop_length=256,\n",
    "            n_mels=n_mels\n",
    "        )\n",
    "\n",
    "        for session in session_ids:\n",
    "            trans_path = os.path.join(root_dir, 'transcriptions', version, f'{session}.json')\n",
    "            with open(trans_path, 'r') as f:\n",
    "                session_trans = json.load(f)\n",
    "            \n",
    "            self.session_data[session] = []\n",
    "            for utt in session_trans:\n",
    "                audio_path = os.path.join(root_dir, 'audio', version,\n",
    "                                        f\"{utt['session_id']}_{utt['speaker_id']}.wav\")\n",
    "                if os.path.exists(audio_path):\n",
    "                    self.session_data[session].append({\n",
    "                        'audio_path': audio_path,\n",
    "                        'start': utt['start_time'],\n",
    "                        'end': utt['end_time']\n",
    "                    })\n",
    "        \n",
    "        self.pairs = []\n",
    "        for session in session_ids:\n",
    "            speakers = self.session_data[session]\n",
    "            for i in range(len(speakers)):\n",
    "                for j in range(i+1, len(speakers)):\n",
    "                    self.pairs.append((session, i, j))\n",
    "                    if len(self.pairs) >= max_pairs:\n",
    "                        break\n",
    "                if len(self.pairs) >= max_pairs:\n",
    "                    break\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        session, i, j = self.pairs[idx]\n",
    "        utt1 = self.session_data[session][i]\n",
    "        utt2 = self.session_data[session][j]\n",
    "\n",
    "        audio1 = self._load_full_audio(utt1['audio_path'])\n",
    "        audio2 = self._load_full_audio(utt2['audio_path'])\n",
    "\n",
    "        min_len = min(audio1.shape[0], audio2.shape[0])\n",
    "\n",
    "        if min_len < self.segment_samples:\n",
    "            audio1 = self._pad_segment(audio1)\n",
    "            audio2 = self._pad_segment(audio2)\n",
    "            segment1 = audio1\n",
    "            segment2 = audio2\n",
    "        else:\n",
    "            max_start = min_len - self.segment_samples\n",
    "            start = random.randint(0, max_start)\n",
    "            segment1 = audio1[start:start + self.segment_samples]\n",
    "            segment2 = audio2[start:start + self.segment_samples]\n",
    "\n",
    "        mel1 = self._to_mel(audio1)\n",
    "        mel2 = self._to_mel(audio2)\n",
    "\n",
    "        mixed = (mel1 + mel2).clamp(min=0)  # Mel spectrograms are non-negative\n",
    "\n",
    "        return mixed, torch.stack([mel1, mel2])  # Shape: [2, n_mels] or [2, n_mels, T]\n",
    "\n",
    "    def _load_full_audio(self, path):\n",
    "        waveform, orig_sr = torchaudio.load(path)\n",
    "        if orig_sr != self.sr:\n",
    "            resampler = torchaudio.transforms.Resample(orig_sr, self.sr)\n",
    "            waveform = resampler(waveform)\n",
    "        waveform = torch.mean(waveform, dim=0)  # Convert to mono\n",
    "        return waveform\n",
    "\n",
    "    def _pad_segment(self, audio):\n",
    "        if len(audio) < self.segment_samples:\n",
    "            padding = self.segment_samples - len(audio)\n",
    "            audio = torch.nn.functional.pad(audio, (0, padding))\n",
    "        else:\n",
    "            audio = audio[:self.segment_samples]\n",
    "        return audio\n",
    "    \n",
    "\n",
    "    def _to_mel(self, audio):\n",
    "        mel = self.mel_transform(audio.unsqueeze(0))  # [1, n_mels, T]\n",
    "        if self.mel_reduction == 'mean':\n",
    "            return mel.mean(dim=-1).squeeze(0)  # [n_mels]\n",
    "        elif self.mel_reduction == 'flatten':\n",
    "            return mel.squeeze(0).flatten() # [n_mels * T]\n",
    "        else:\n",
    "            return mel.squeeze(0) # [n_mels, T]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeparationKAN(nn.Module):\n",
    "    def __init__(self, input_size=48000, encoder:list[int]=[256, 256], latent_dim=256, decoder:list[int] = [256, 256]):\n",
    "        super().__init__()\n",
    "        self.encoder = KAN(width=[input_size, *encoder, latent_dim])\n",
    "        self.decoder1 = KAN(width=[latent_dim//2, *decoder, input_size])\n",
    "        self.decoder2 = KAN(width=[latent_dim//2, *decoder, input_size])\n",
    "\n",
    "    def forward(self, x):\n",
    "        latent = self.encoder(x)\n",
    "        latent1, latent2 = torch.chunk(latent, 2, dim=-1)\n",
    "        return torch.stack([\n",
    "            self.decoder1(latent1),\n",
    "            self.decoder2(latent2)\n",
    "        ], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PermutationInvariantLoss(nn.Module):\n",
    "    def __init__(self, base_loss=nn.L1Loss()):\n",
    "        super().__init__()\n",
    "        self.base_loss = base_loss\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        loss_a = self.base_loss(outputs[:,0], targets[:,0]) + \\\n",
    "                 self.base_loss(outputs[:,1], targets[:,1])\n",
    "        loss_b = self.base_loss(outputs[:,0], targets[:,1]) + \\\n",
    "                 self.base_loss(outputs[:,1], targets[:,0])\n",
    "        return torch.min(loss_a, loss_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1004\n",
      "torch.Size([1, 128]) torch.Size([1, 2, 128])\n",
      "tensor([[2.3471e-04, 7.8877e-04, 1.7647e-03, 2.8701e-03, 4.1116e-03, 8.3375e-03,\n",
      "         3.6199e-02, 1.7696e-01, 4.4592e-01, 6.2209e-01, 6.2036e-01, 4.8691e-01,\n",
      "         3.5304e-01, 2.6162e-01, 2.3135e-01, 2.7661e-01, 3.0550e-01, 3.3568e-01,\n",
      "         3.3718e-01, 3.0596e-01, 2.9672e-01, 2.9946e-01, 2.4864e-01, 3.0665e-01,\n",
      "         3.5024e-01, 3.4213e-01, 3.5246e-01, 3.1802e-01, 3.5571e-01, 2.6817e-01,\n",
      "         2.7617e-01, 2.8831e-01, 2.4807e-01, 2.3035e-01, 2.1701e-01, 1.5719e-01,\n",
      "         9.3636e-02, 8.2106e-02, 6.6607e-02, 6.7845e-02, 4.9850e-02, 3.8268e-02,\n",
      "         3.5620e-02, 3.2617e-02, 3.8799e-02, 2.9091e-02, 2.5990e-02, 2.6568e-02,\n",
      "         2.5496e-02, 2.4307e-02, 2.8702e-02, 3.3976e-02, 2.4577e-02, 2.3908e-02,\n",
      "         2.5708e-02, 2.6513e-02, 2.6127e-02, 2.4911e-02, 2.6735e-02, 2.2643e-02,\n",
      "         2.1462e-02, 2.1346e-02, 2.0697e-02, 1.7069e-02, 1.3754e-02, 1.2777e-02,\n",
      "         1.1058e-02, 7.7549e-03, 6.3068e-03, 6.1289e-03, 4.9988e-03, 4.7367e-03,\n",
      "         4.6080e-03, 4.9469e-03, 4.6197e-03, 3.8484e-03, 3.8532e-03, 4.0535e-03,\n",
      "         4.0821e-03, 5.0888e-03, 4.6020e-03, 4.7564e-03, 5.2059e-03, 5.0060e-03,\n",
      "         4.4902e-03, 3.7585e-03, 3.7897e-03, 3.2984e-03, 3.1414e-03, 3.2913e-03,\n",
      "         3.1598e-03, 3.2829e-03, 3.4343e-03, 3.6995e-03, 3.9396e-03, 3.9936e-03,\n",
      "         3.9711e-03, 3.3183e-03, 2.5171e-03, 1.6478e-03, 1.2668e-03, 1.4368e-03,\n",
      "         1.8680e-03, 2.0267e-03, 1.7611e-03, 1.6364e-03, 1.5933e-03, 1.5408e-03,\n",
      "         1.4503e-03, 1.2301e-03, 1.0299e-03, 9.0578e-04, 7.8583e-04, 7.3786e-04,\n",
      "         7.3336e-04, 6.8448e-04, 6.3435e-04, 5.6354e-04, 5.7423e-04, 6.2319e-04,\n",
      "         6.5058e-04, 7.3985e-04, 8.2166e-04, 7.1977e-04, 6.3006e-04, 4.9615e-04,\n",
      "         2.1348e-04, 1.3941e-05]]) tensor([[[7.3076e-05, 3.2474e-04, 7.3483e-04, 1.1606e-03, 1.6973e-03,\n",
      "          4.3345e-03, 1.1193e-02, 1.8194e-02, 2.4992e-02, 3.2957e-02,\n",
      "          3.2832e-02, 2.4939e-02, 2.5411e-02, 3.0047e-02, 3.2987e-02,\n",
      "          3.0540e-02, 2.0046e-02, 1.5501e-02, 1.5540e-02, 1.5775e-02,\n",
      "          1.7563e-02, 2.0488e-02, 1.6649e-02, 1.9567e-02, 2.1849e-02,\n",
      "          2.1394e-02, 2.0710e-02, 1.3820e-02, 1.3974e-02, 1.2069e-02,\n",
      "          1.1130e-02, 9.5467e-03, 5.9941e-03, 5.0211e-03, 3.4413e-03,\n",
      "          1.9912e-03, 1.2544e-03, 1.0344e-03, 9.7949e-04, 8.7549e-04,\n",
      "          5.5962e-04, 5.0469e-04, 7.5438e-04, 9.0036e-04, 1.0786e-03,\n",
      "          8.5081e-04, 6.1947e-04, 6.8505e-04, 8.0468e-04, 7.6263e-04,\n",
      "          7.1742e-04, 5.5425e-04, 5.1179e-04, 5.9762e-04, 6.8257e-04,\n",
      "          9.3610e-04, 1.1516e-03, 8.4796e-04, 7.3142e-04, 5.8409e-04,\n",
      "          5.4149e-04, 5.4035e-04, 4.7819e-04, 4.4833e-04, 3.8308e-04,\n",
      "          3.1619e-04, 2.8858e-04, 2.4645e-04, 2.7591e-04, 2.3300e-04,\n",
      "          2.2317e-04, 2.3016e-04, 2.4177e-04, 3.0889e-04, 3.7758e-04,\n",
      "          3.6786e-04, 3.0718e-04, 3.0904e-04, 3.8868e-04, 9.7382e-04,\n",
      "          4.4336e-04, 2.6489e-04, 3.1739e-04, 2.2954e-04, 1.6675e-04,\n",
      "          1.6511e-04, 1.4739e-04, 1.4893e-04, 1.6354e-04, 1.7437e-04,\n",
      "          1.7795e-04, 1.5728e-04, 1.4300e-04, 1.4342e-04, 1.3957e-04,\n",
      "          1.5917e-04, 1.7286e-04, 2.4193e-04, 2.1019e-04, 1.4964e-04,\n",
      "          1.4831e-04, 1.4506e-04, 1.8632e-04, 1.8235e-04, 1.3427e-04,\n",
      "          1.1398e-04, 1.0174e-04, 1.1539e-04, 1.4151e-04, 1.2464e-04,\n",
      "          8.1550e-05, 7.9318e-05, 7.3337e-05, 1.0467e-04, 1.3537e-04,\n",
      "          8.4017e-05, 8.1752e-05, 8.3251e-05, 1.0070e-04, 1.0172e-04,\n",
      "          9.1682e-05, 7.9159e-05, 9.0043e-05, 1.0882e-04, 1.0424e-04,\n",
      "          6.4782e-05, 2.8986e-05, 2.2367e-06],\n",
      "         [1.6164e-04, 4.6403e-04, 1.0298e-03, 1.7095e-03, 2.4143e-03,\n",
      "          4.0030e-03, 2.5006e-02, 1.5876e-01, 4.2093e-01, 5.8913e-01,\n",
      "          5.8753e-01, 4.6197e-01, 3.2762e-01, 2.3158e-01, 1.9836e-01,\n",
      "          2.4607e-01, 2.8545e-01, 3.2018e-01, 3.2164e-01, 2.9019e-01,\n",
      "          2.7916e-01, 2.7897e-01, 2.3199e-01, 2.8709e-01, 3.2839e-01,\n",
      "          3.2074e-01, 3.3175e-01, 3.0420e-01, 3.4174e-01, 2.5610e-01,\n",
      "          2.6504e-01, 2.7876e-01, 2.4207e-01, 2.2533e-01, 2.1357e-01,\n",
      "          1.5520e-01, 9.2382e-02, 8.1072e-02, 6.5627e-02, 6.6969e-02,\n",
      "          4.9290e-02, 3.7763e-02, 3.4866e-02, 3.1717e-02, 3.7721e-02,\n",
      "          2.8240e-02, 2.5370e-02, 2.5883e-02, 2.4691e-02, 2.3545e-02,\n",
      "          2.7985e-02, 3.3422e-02, 2.4066e-02, 2.3310e-02, 2.5026e-02,\n",
      "          2.5577e-02, 2.4976e-02, 2.4063e-02, 2.6003e-02, 2.2059e-02,\n",
      "          2.0921e-02, 2.0805e-02, 2.0219e-02, 1.6620e-02, 1.3371e-02,\n",
      "          1.2461e-02, 1.0769e-02, 7.5084e-03, 6.0309e-03, 5.8959e-03,\n",
      "          4.7756e-03, 4.5065e-03, 4.3662e-03, 4.6380e-03, 4.2421e-03,\n",
      "          3.4805e-03, 3.5460e-03, 3.7444e-03, 3.6934e-03, 4.1150e-03,\n",
      "          4.1586e-03, 4.4915e-03, 4.8885e-03, 4.7764e-03, 4.3235e-03,\n",
      "          3.5934e-03, 3.6424e-03, 3.1495e-03, 2.9779e-03, 3.1170e-03,\n",
      "          2.9819e-03, 3.1256e-03, 3.2913e-03, 3.5561e-03, 3.8000e-03,\n",
      "          3.8344e-03, 3.7982e-03, 3.0763e-03, 2.3069e-03, 1.4982e-03,\n",
      "          1.1185e-03, 1.2917e-03, 1.6816e-03, 1.8443e-03, 1.6268e-03,\n",
      "          1.5224e-03, 1.4915e-03, 1.4254e-03, 1.3088e-03, 1.1055e-03,\n",
      "          9.4834e-04, 8.2646e-04, 7.1249e-04, 6.3319e-04, 5.9799e-04,\n",
      "          6.0047e-04, 5.5260e-04, 4.8029e-04, 4.7353e-04, 5.2147e-04,\n",
      "          5.5890e-04, 6.6069e-04, 7.3162e-04, 6.1096e-04, 5.2582e-04,\n",
      "          4.3137e-04, 1.8450e-04, 1.1704e-05]]])\n"
     ]
    }
   ],
   "source": [
    "dataset = DiPCoSeparationDataset(\n",
    "    root_dir='./data/Dipco/',\n",
    "    session_ids=['S02', 'S04', 'S05', 'S09', 'S10'],\n",
    "    segment_length=3,\n",
    ")\n",
    "train_loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "print(len(train_loader))\n",
    "for mixed, clean in train_loader:\n",
    "    print(mixed.shape, clean.shape)\n",
    "    print(mixed, clean)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chaud\\Desktop\\!Back_this_folder!\\College\\Masters\\Spring 2025\\Neural Networks Deep Learning\\Final Project\\.venv\\lib\\site-packages\\kan\\MultKAN.py:813: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\ReduceOps.cpp:1839.)\n",
      "  self.subnode_actscale.append(torch.std(x, dim=0).detach())\n",
      "c:\\Users\\chaud\\Desktop\\!Back_this_folder!\\College\\Masters\\Spring 2025\\Neural Networks Deep Learning\\Final Project\\.venv\\lib\\site-packages\\kan\\MultKAN.py:823: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\ReduceOps.cpp:1839.)\n",
      "  input_range = torch.std(preacts, dim=0) + 0.1\n",
      "c:\\Users\\chaud\\Desktop\\!Back_this_folder!\\College\\Masters\\Spring 2025\\Neural Networks Deep Learning\\Final Project\\.venv\\lib\\site-packages\\kan\\MultKAN.py:824: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\ReduceOps.cpp:1839.)\n",
      "  output_range_spline = torch.std(postacts_numerical, dim=0) # for training, only penalize the spline part\n",
      "c:\\Users\\chaud\\Desktop\\!Back_this_folder!\\College\\Masters\\Spring 2025\\Neural Networks Deep Learning\\Final Project\\.venv\\lib\\site-packages\\kan\\MultKAN.py:825: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\ReduceOps.cpp:1839.)\n",
      "  output_range = torch.std(postacts, dim=0) # for visualization, include the contribution from both spline + symbolic\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\chaud\\Desktop\\!Back_this_folder!\\College\\Masters\\Spring 2025\\Neural Networks Deep Learning\\Final Project\\KAN_cocktail.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/chaud/Desktop/%21Back_this_folder%21/College/Masters/Spring%202025/Neural%20Networks%20Deep%20Learning/Final%20Project/KAN_cocktail.ipynb#W6sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m mixed, clean \u001b[39min\u001b[39;00m train_loader:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/chaud/Desktop/%21Back_this_folder%21/College/Masters/Spring%202025/Neural%20Networks%20Deep%20Learning/Final%20Project/KAN_cocktail.ipynb#W6sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/chaud/Desktop/%21Back_this_folder%21/College/Masters/Spring%202025/Neural%20Networks%20Deep%20Learning/Final%20Project/KAN_cocktail.ipynb#W6sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     outputs \u001b[39m=\u001b[39m model(mixed)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chaud/Desktop/%21Back_this_folder%21/College/Masters/Spring%202025/Neural%20Networks%20Deep%20Learning/Final%20Project/KAN_cocktail.ipynb#W6sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     loss \u001b[39m=\u001b[39m criterion(outputs, clean)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chaud/Desktop/%21Back_this_folder%21/College/Masters/Spring%202025/Neural%20Networks%20Deep%20Learning/Final%20Project/KAN_cocktail.ipynb#W6sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39mprint\u001b[39m(loss)\n",
      "File \u001b[1;32mc:\\Users\\chaud\\Desktop\\!Back_this_folder!\\College\\Masters\\Spring 2025\\Neural Networks Deep Learning\\Final Project\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\chaud\\Desktop\\!Back_this_folder!\\College\\Masters\\Spring 2025\\Neural Networks Deep Learning\\Final Project\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "\u001b[1;32mc:\\Users\\chaud\\Desktop\\!Back_this_folder!\\College\\Masters\\Spring 2025\\Neural Networks Deep Learning\\Final Project\\KAN_cocktail.ipynb Cell 9\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/chaud/Desktop/%21Back_this_folder%21/College/Masters/Spring%202025/Neural%20Networks%20Deep%20Learning/Final%20Project/KAN_cocktail.ipynb#W6sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/chaud/Desktop/%21Back_this_folder%21/College/Masters/Spring%202025/Neural%20Networks%20Deep%20Learning/Final%20Project/KAN_cocktail.ipynb#W6sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     latent \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chaud/Desktop/%21Back_this_folder%21/College/Masters/Spring%202025/Neural%20Networks%20Deep%20Learning/Final%20Project/KAN_cocktail.ipynb#W6sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     latent1, latent2 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mchunk(latent, \u001b[39m2\u001b[39m, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chaud/Desktop/%21Back_this_folder%21/College/Masters/Spring%202025/Neural%20Networks%20Deep%20Learning/Final%20Project/KAN_cocktail.ipynb#W6sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mstack([\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chaud/Desktop/%21Back_this_folder%21/College/Masters/Spring%202025/Neural%20Networks%20Deep%20Learning/Final%20Project/KAN_cocktail.ipynb#W6sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder1(latent1),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chaud/Desktop/%21Back_this_folder%21/College/Masters/Spring%202025/Neural%20Networks%20Deep%20Learning/Final%20Project/KAN_cocktail.ipynb#W6sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder2(latent2)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chaud/Desktop/%21Back_this_folder%21/College/Masters/Spring%202025/Neural%20Networks%20Deep%20Learning/Final%20Project/KAN_cocktail.ipynb#W6sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     ], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\chaud\\Desktop\\!Back_this_folder!\\College\\Masters\\Spring 2025\\Neural Networks Deep Learning\\Final Project\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\chaud\\Desktop\\!Back_this_folder!\\College\\Masters\\Spring 2025\\Neural Networks Deep Learning\\Final Project\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\chaud\\Desktop\\!Back_this_folder!\\College\\Masters\\Spring 2025\\Neural Networks Deep Learning\\Final Project\\.venv\\lib\\site-packages\\kan\\MultKAN.py:804\u001b[0m, in \u001b[0;36mMultKAN.forward\u001b[1;34m(self, x, singularity_avoiding, y_th)\u001b[0m\n\u001b[0;32m    801\u001b[0m \u001b[39m#print(preacts, postacts_numerical, postspline)\u001b[39;00m\n\u001b[0;32m    803\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msymbolic_enabled \u001b[39m==\u001b[39m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 804\u001b[0m     x_symbolic, postacts_symbolic \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msymbolic_fun[l](x, singularity_avoiding\u001b[39m=\u001b[39;49msingularity_avoiding, y_th\u001b[39m=\u001b[39;49my_th)\n\u001b[0;32m    805\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    806\u001b[0m     x_symbolic \u001b[39m=\u001b[39m \u001b[39m0.\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\chaud\\Desktop\\!Back_this_folder!\\College\\Masters\\Spring 2025\\Neural Networks Deep Learning\\Final Project\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\chaud\\Desktop\\!Back_this_folder!\\College\\Masters\\Spring 2025\\Neural Networks Deep Learning\\Final Project\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\chaud\\Desktop\\!Back_this_folder!\\College\\Masters\\Spring 2025\\Neural Networks Deep Learning\\Final Project\\.venv\\lib\\site-packages\\kan\\Symbolic_KANLayer.py:116\u001b[0m, in \u001b[0;36mSymbolic_KANLayer.forward\u001b[1;34m(self, x, singularity_avoiding, y_th)\u001b[0m\n\u001b[0;32m    114\u001b[0m         xij \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maffine[j,i,\u001b[39m2\u001b[39m]\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfuns_avoid_singularity[j][i](\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maffine[j,i,\u001b[39m0\u001b[39m]\u001b[39m*\u001b[39mx[:,[i]]\u001b[39m+\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maffine[j,i,\u001b[39m1\u001b[39m], torch\u001b[39m.\u001b[39mtensor(y_th))[\u001b[39m1\u001b[39m]\u001b[39m+\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maffine[j,i,\u001b[39m3\u001b[39m]\n\u001b[0;32m    115\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 116\u001b[0m         xij \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maffine[j,i,\u001b[39m2\u001b[39m]\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfuns[j][i](\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maffine[j,i,\u001b[39m0\u001b[39m]\u001b[39m*\u001b[39mx[:,[i]]\u001b[39m+\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maffine[j,i,\u001b[39m1\u001b[39m])\u001b[39m+\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maffine[j,i,\u001b[39m3\u001b[39m]\n\u001b[0;32m    117\u001b[0m     postacts_\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmask[j][i]\u001b[39m*\u001b[39mxij)\n\u001b[0;32m    118\u001b[0m postacts\u001b[39m.\u001b[39mappend(torch\u001b[39m.\u001b[39mstack(postacts_))\n",
      "File \u001b[1;32mc:\\Users\\chaud\\Desktop\\!Back_this_folder!\\College\\Masters\\Spring 2025\\Neural Networks Deep Learning\\Final Project\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1927\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1922\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39m=\u001b[39m OrderedDict()\n\u001b[0;32m   1924\u001b[0m \u001b[39m# It is crucial that the return type is not annotated as `Any`, otherwise type checking\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \u001b[39m# on `torch.nn.Module` and all its subclasses is largely disabled as a result. See:\u001b[39;00m\n\u001b[0;32m   1926\u001b[0m \u001b[39m# https://github.com/pytorch/pytorch/pull/115074\u001b[39;00m\n\u001b[1;32m-> 1927\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m__getattr__\u001b[39m(\u001b[39mself\u001b[39m, name: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[Tensor, \u001b[39m\"\u001b[39m\u001b[39mModule\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m   1928\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m_parameters\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m:\n\u001b[0;32m   1929\u001b[0m         _parameters \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39m_parameters\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = SeparationKAN(input_size=128)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "criterion = PermutationInvariantLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1):\n",
    "    epoch_loss = 0\n",
    "    for mixed, clean in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(mixed)\n",
    "        loss = criterion(outputs, clean)\n",
    "        print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {epoch_loss/len(train_loader):.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
