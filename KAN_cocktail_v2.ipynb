{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "import torchaudio\n",
    "from kan import KAN\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torchaudio.transforms as T\n",
    "import random\n",
    "\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_size(model):\n",
    "    param_size = 0\n",
    "    buffer_size = 0\n",
    "    total_params = 0\n",
    "    trainable_params = 0\n",
    "\n",
    "    for param in model.parameters():\n",
    "        total_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "        param_size += param.numel() * param.element_size()  # size in bytes\n",
    "\n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.numel() * buffer.element_size()\n",
    "\n",
    "    total_size_mb = (param_size + buffer_size) / (1024 ** 2)  # Convert bytes to MB\n",
    "\n",
    "    print(f\"Model size       : {total_size_mb:.4f} MB\")\n",
    "    print(f\"Total parameters : {total_params:,}\")\n",
    "    print(f\"Trainable params : {trainable_params:,}\")\n",
    "\n",
    "    return {\n",
    "        \"size_mb\": total_size_mb,\n",
    "        \"total_params\": total_params,\n",
    "        \"trainable_params\": trainable_params\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_waveform(waveform, sr, title=\"Waveform\", ax=None):\n",
    "    waveform = waveform.numpy()\n",
    "\n",
    "    num_channels, num_frames = waveform.shape\n",
    "    time_axis = torch.arange(0, num_frames) / sr\n",
    "\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots(num_channels, 1)\n",
    "    ax.plot(time_axis, waveform[0], linewidth=1)\n",
    "    ax.grid(True)\n",
    "    ax.set_xlim([0, time_axis[-1]])\n",
    "    ax.set_title(title)\n",
    "\n",
    "def plot_mono_waveform(waveform, sr, title=\"Waveform\", ax=None):\n",
    "    waveform = waveform.numpy()\n",
    "\n",
    "    num_channels, num_frames = 1, waveform.shape[0]\n",
    "    time_axis = torch.arange(0, num_frames) / sr\n",
    "\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots(num_channels, 1)\n",
    "    ax.plot(time_axis, waveform, linewidth=1)\n",
    "    ax.grid(True)\n",
    "    ax.set_xlim([0, time_axis[-1]])\n",
    "    ax.set_title(title)\n",
    "\n",
    "\n",
    "def plot_spectrogram(specgram, title=None, ylabel=\"freq_bin\", ax=None):\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots(1, 1)\n",
    "    if title is not None:\n",
    "        ax.set_title(title)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.imshow(librosa.power_to_db(specgram), origin=\"lower\", aspect=\"auto\", interpolation=\"nearest\")\n",
    "\n",
    "\n",
    "def plot_fbank(fbank, title=None):\n",
    "    fig, axs = plt.subplots(1, 1)\n",
    "    axs.set_title(title or \"Filter bank\")\n",
    "    axs.imshow(fbank, aspect=\"auto\")\n",
    "    axs.set_ylabel(\"frequency bin\")\n",
    "    axs.set_xlabel(\"mel bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "class DiPCoSeparationDataset(Dataset):\n",
    "    def __init__(self, root_dir='./data/Dipco/', session_ids=['S02', 'S04'], version='dev', \n",
    "                segment_length=4, sr=16000, max_pairs=10000, seed=0):\n",
    "        \n",
    "        self.sr = sr\n",
    "        self.segment_samples = int(sr * segment_length)\n",
    "        \n",
    "        self.session_dict:dict[list] = {}\n",
    "        self.pairs = []\n",
    "        self.session_audio:dict[tuple[str, str],torch.Tensor] = {}\n",
    "        \n",
    "\n",
    "        for session_id in session_ids:\n",
    "            trans_path = os.path.join(root_dir, 'transcriptions', version, f'{session_id}.json')\n",
    "            print('Processing: ', trans_path)\n",
    "            with open(trans_path, 'r') as f:\n",
    "                session_trans = json.load(f)\n",
    "\n",
    "            for utt in tqdm(session_trans):\n",
    "                curr_speaker_id = utt['speaker_id']\n",
    "                curr_valid_times = self.session_dict.get((session_id, curr_speaker_id), [])\n",
    "                curr_start_time = self._time_str_to_sample(utt['start_time'], sr)\n",
    "                curr_end_time = self._time_str_to_sample(utt['end_time'], sr)\n",
    "                \n",
    "                curr_valid_times.extend([(start, start + segment_length * sr) for start in range(curr_start_time, curr_end_time, self.segment_samples)])\n",
    "                self.session_dict[(session_id, curr_speaker_id)] = curr_valid_times\n",
    "\n",
    "\n",
    "            for speaker_1 in list(self.session_dict.keys())[:-1]:\n",
    "                if speaker_1[0] != session_id:\n",
    "                    continue\n",
    "                speaker_1_times = self.session_dict[speaker_1]\n",
    "                for speaker_2 in list(self.session_dict)[1:]:\n",
    "                    if speaker_2[0] != session_id:\n",
    "                        continue\n",
    "                    speaker_2_times = self.session_dict[speaker_2]\n",
    "                    for time1 in speaker_1_times:\n",
    "                        for time2 in speaker_2_times:\n",
    "                            self.pairs.append((speaker_1, time1, speaker_2, time2))\n",
    "                            \n",
    "            random.seed(seed)  \n",
    "            random.shuffle(self.pairs)\n",
    "            self.pairs = self.pairs[:max_pairs]\n",
    "\n",
    "        for (key_session_id, key_speaker_id), times in self.session_dict.items():\n",
    "            print((key_session_id, key_speaker_id), len(times), times)\n",
    "            audio_path = os.path.join(root_dir, 'audio', version,\n",
    "                                    f\"{key_session_id}_{key_speaker_id}.wav\")\n",
    "            self.session_audio[(key_session_id, key_speaker_id)] = self._load_full_audio(audio_path)\n",
    "\n",
    "            print((key_session_id, key_speaker_id),'audio length:',len(self.session_audio[(key_session_id, key_speaker_id)]))\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        (speaker_1, time1, speaker_2, time2) = self.pairs[idx]\n",
    "        audio1 = self.session_audio[speaker_1]\n",
    "        audio2 = self.session_audio[speaker_2]\n",
    "        if time1[1] > audio1.size(0):\n",
    "            time1 = (audio1.size(0) - self.segment_samples, audio1.size(0))\n",
    "        if time2[1] > audio2.size(0):\n",
    "            time2 = (audio2.size(0) - self.segment_samples, audio2.size(0))\n",
    "        clean1 = audio1[time1[0]:time1[1]]\n",
    "        clean2 = audio2[time2[0]:time2[1]]\n",
    "\n",
    "        mixed = (clean1 + clean2).clamp(-1, 1)\n",
    "\n",
    "        clean1 = clean1.unsqueeze(0)\n",
    "        clean2 = clean2.unsqueeze(0)\n",
    "        mixed = mixed.unsqueeze(0)\n",
    "\n",
    "        return mixed, torch.stack([clean1, clean2])\n",
    "\n",
    "    def _time_str_to_sample(self, t_dict, sr):\n",
    "        t_str = t_dict[list(t_dict.keys())[0]]\n",
    "        t = datetime.strptime(t_str, \"%H:%M:%S.%f\")\n",
    "        return int(sr * (t.hour * 3600 + t.minute * 60 + t.second + t.microsecond / 1_000_000))\n",
    "    \n",
    "    def _load_full_audio(self, path):\n",
    "        waveform, orig_sr = torchaudio.load(path)\n",
    "        if orig_sr != self.sr:\n",
    "            resampler = torchaudio.transforms.Resample(orig_sr, self.sr)\n",
    "            waveform = resampler(waveform)\n",
    "        waveform = torch.mean(waveform, dim=0)  # Convert to mono\n",
    "        return waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DiPCoSeparationDataset(\n",
    "    root_dir='./data/Dipco/',\n",
    "    session_ids=['S02', 'S04'],#, 'S04', 'S05', 'S09', 'S10'],\n",
    "    segment_length=4,\n",
    "    sr=8000,\n",
    "    max_pairs=1_000_000,\n",
    ")\n",
    "len(train_dataset)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "len(train_dataset.pairs), train_dataset.pairs[0], len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = DiPCoSeparationDataset(\n",
    "    root_dir='./data/Dipco/',\n",
    "    version='eval',\n",
    "    session_ids=['S01', 'S03'],\n",
    "    segment_length=4,\n",
    "    sr=8000,\n",
    "    max_pairs=len(train_dataset.pairs) // 10,\n",
    ")\n",
    "len(test_dataset)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)\n",
    "len(test_dataset.pairs), test_dataset.pairs[0], len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (mixed, clean) in enumerate(train_loader):\n",
    "    print(idx, mixed.shape, clean.shape, end='\\r')\n",
    "    if idx > 1000:\n",
    "        print()\n",
    "        break\n",
    "for idx, (mixed, clean) in enumerate(test_loader):\n",
    "    print(idx, mixed.shape, clean.shape, end='\\r')\n",
    "    if idx > 1000:\n",
    "        print()\n",
    "        break\n",
    "\n",
    "for mixed, clean in train_loader:\n",
    "    print(mixed.shape, clean.shape)\n",
    "    plot_waveform(mixed[0], sr=16000)\n",
    "    plot_waveform(clean[0][0], sr=16000)\n",
    "    plot_waveform(clean[0][1], sr=16000)\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PermutationInvariantSISNRLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-8\n",
    "\n",
    "    def _si_snr(self, estimates, targets):\n",
    "        # estimates: [B, T]\n",
    "        # targets: [B, T]\n",
    "        estimates = estimates - estimates.mean(dim=-1, keepdim=True)\n",
    "        targets = targets - targets.mean(dim=-1, keepdim=True)\n",
    "\n",
    "        # Compute SI-SNR\n",
    "        s_target = (torch.sum(estimates * targets, dim=-1, keepdim=True) * targets) \\\n",
    "                   / (torch.norm(targets, p=2, dim=-1, keepdim=True)**2 + self.eps)\n",
    "        \n",
    "        e_noise = estimates - s_target\n",
    "        return 10 * torch.log10(\n",
    "            (torch.norm(s_target, p=2, dim=-1)**2 + self.eps) / \n",
    "            (torch.norm(e_noise, p=2, dim=-1)**2 + self.eps)\n",
    "        )\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            outputs: [B, 2, T] separated waveforms\n",
    "            targets: [B, 2, T] original clean waveforms\n",
    "        Returns:\n",
    "            loss: scalar\n",
    "        \"\"\"\n",
    "        # Compute all permutations\n",
    "        loss_0 = self._si_snr(outputs[:,0], targets[:,0]) + \\\n",
    "                 self._si_snr(outputs[:,1], targets[:,1])\n",
    "        \n",
    "        loss_1 = self._si_snr(outputs[:,0], targets[:,1]) + \\\n",
    "                 self._si_snr(outputs[:,1], targets[:,0])\n",
    "\n",
    "        # Return negative of best permutation\n",
    "        return -torch.mean(torch.min(loss_0, loss_1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchaudio.models import ConvTasNet\n",
    "\n",
    "model = ConvTasNet(num_sources=2, enc_kernel_size=16, enc_num_feats=16,msk_kernel_size=3, msk_num_feats=8, msk_num_hidden_feats=16, msk_num_layers=3, msk_num_stacks=3).to(device)\n",
    "get_model_size(model)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "criterion  = PermutationInvariantSISNRLoss()\n",
    "\n",
    "for epoch in range(2):\n",
    "    epoch_loss = 0\n",
    "    for s_idx, (mixed, clean) in tqdm(enumerate(train_loader), desc=f'Epoch {epoch + 1}', position=epoch, total=len(train_loader)):\n",
    "        model.train()\n",
    "        # print(mixed.shape, clean.shape)\n",
    "        mixed = mixed.to(device)\n",
    "        clean = clean.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(mixed)\n",
    "        # print(outputs.shape)\n",
    "        loss = criterion(outputs, clean)\n",
    "        # print(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        if s_idx % 1000 == 0:\n",
    "            full_val_loss = 0\n",
    "            for test_idx, (val_mixed, val_clean) in enumerate(test_loader):\n",
    "                model.eval()\n",
    "                print('Testing', test_idx ,'/', len(test_loader), end='\\r')\n",
    "                val_mixed = val_mixed.to(device)\n",
    "                val_clean = val_clean.to(device)\n",
    "                val_outputs = model(val_mixed)\n",
    "                val_loss = criterion(val_outputs, val_clean)\n",
    "                full_val_loss += val_loss.item()\n",
    "            checkpoint_path = f'./models/simple_convtasnet_epoch{epoch+1}_{s_idx}.pth'\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            print(f\"Saved model checkpoint to {checkpoint_path}, Loss: {epoch_loss/s_idx}, Val_Loss: {full_val_loss/len(test_loader)}\")\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {epoch_loss/len(train_loader):.4f}\")\n",
    "    checkpoint_path = f'./models/simple_convtasnet_epoch{epoch+1}_final.pth'\n",
    "    torch.save(model.state_dict(), checkpoint_path)\n",
    "    print(f\"Saved model checkpoint to {checkpoint_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
